{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tracyhuang/Documents/Berkeley/FinalProject\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "new_dir = cwd + '/cnn/*.story'\n",
    "files = glob.glob(new_dir)\n",
    "\n",
    "for myfile in files:\n",
    "    with open(myfile) as f:\n",
    "        raw_docs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "#print tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "\n",
    "tokenized_docs_wo_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    \n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_wo_punctuation.append(new_review)\n",
    "    \n",
    "#print tokenized_docs_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tracyhuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenized_docs_wo_stopwords = []\n",
    "for doc in tokenized_docs_wo_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    tokenized_docs_wo_stopwords.append(new_term_vector)\n",
    "            \n",
    "#print tokenized_docs_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_wo_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "#print preprocessed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from operator import attrgetter\n",
    "\n",
    "class TextSummarization():\n",
    "    def __init__(self):\n",
    "        if not callable(stemmer):\n",
    "            raise ValueError(\"Stemmer needs to be callable\")\n",
    "\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def StopWords(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @StopWords.setter\n",
    "    def StopWords(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def call(self, document, sentences_count):\n",
    "        ratings = self.rate_sentences(document)\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "    \n",
    "    def StemWord(self, word):\n",
    "        return self._stemmer(self.normalize_word(word))\n",
    "\n",
    "    def Normalization(self, word):\n",
    "        return to_unicode(word).lower()\n",
    "\n",
    "    def TopSentences(self, sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            rate = lambda s: rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "            for o, s in enumerate(sentences))\n",
    "\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        if not isinstance(count, ItemsCount):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "\n",
    "        return tuple(i.sentence for i in infos)\n",
    "\n",
    "    def RateSentences(self, document):\n",
    "        sentences_words = [(s, self._to_words_set(s)) for s in document.sentences]\n",
    "        ratings = defaultdict(float)\n",
    "\n",
    "        for (sentence1, words1), (sentence2, words2) in combinations(sentences_words, 2):\n",
    "            rank = self._rate_sentences_edge(words1, words2)\n",
    "            ratings[sentence1] += rank\n",
    "            ratings[sentence2] += rank\n",
    "\n",
    "        return ratings\n",
    "\n",
    "    def WordSet(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    def RateSentencesEdge(self, words1, words2):\n",
    "        rank = 0\n",
    "        for w1 in words1:\n",
    "            for w2 in words2:\n",
    "                rank += int(w1 == w2)\n",
    "\n",
    "        if rank == 0:\n",
    "            return 0.0\n",
    "\n",
    "        assert len(words1) > 0 and len(words2) > 0\n",
    "        norm = math.log(len(words1)) + math.log(len(words2))\n",
    "        return 0.0 if norm == 0.0 else rank / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
